apiVersion: monitoring.coreos.com/v1
kind: PrometheusRule
metadata:
  labels:
    prometheus: k8s
    role: alert-rules
  name: prometheus-k8s-custom-rules
  namespace: openshift-monitoring
spec:
  groups:
  - name: custom_node_monitoring
    rules:
    - alert: node_ntp_timestamp
      expr: abs(node_time_seconds - timestamp(node_time_seconds))  > 1
      annotations:
        severity: "HIGH"
        message: "{{$labels.instance}}: NTP out of sync (current value is: {{$value}})"

    - alert: node_filesystem_free_dev_percent
      expr: node_filesystem_free_bytes / node_filesystem_size_bytes{device!~"/dev.*pv.*",device!~"/dev.*brick.*",device=~"/dev.*"} * 100 < 10
      annotations:
        severity: "HIGH"
        message: "{{$labels.instance}}: Available Space on {{$labels.mountpoint}} (current value is: {{$value}})%"

    - alert: kube_node_status_limit_cpu_percent
      expr: sum(kube_pod_container_resource_limits_cpu_cores * on(pod, namespace) group_left kube_pod_status_phase{phase="Running"}) by (node) / sum(kube_node_status_allocatable_cpu_cores) BY (node) * 100 > 700
      annotations:
        severity: "HIGH"
        message: "{{$labels.node}}: CPU Over-Allocation higher than {{$value}}%"

    - alert: kube_node_status_allocatable_cpu_percent
      expr: sum(kube_pod_container_resource_requests_cpu_cores * on(pod, namespace) group_left kube_pod_status_phase{phase="Running"}) by (node) / sum(kube_node_status_allocatable_cpu_cores) BY (node) * 100 > 98
      for: 30m
      annotations:
        severity: "HIGH"
        message: "{{$labels.node}}: More than 98% of all cores requested (current value is: {{$value}})%"

    - alert: kube_node_status_limit_mem_percent
      expr: sum(kube_pod_container_resource_limits_memory_bytes * on(pod, namespace) group_left kube_pod_status_phase{phase="Running"}) by (node) / sum(kube_node_status_allocatable_memory_bytes) BY (node) * 100 > 700
      annotations:
        severity: "HIGH"
        message: "{{$labels.node}}: Memory Over-Allocation higher than {{$value}}%"

    - alert: kube_node_status_allocatable_mem_percent
      expr: sum(kube_pod_container_resource_requests_memory_bytes * on(pod, namespace) group_left kube_pod_status_phase{phase="Running"}) by (node)/ sum(kube_node_status_allocatable_memory_bytes) BY (node) * 100 > 98
      for: 30m
      annotations:
        severity: "HIGH"
        message: "{{$labels.node}}: More than 98% of memory requested (current value is: {{$value}})%"

    - alert: kube_persistentvolumeclaim_status
      expr: rate(kube_persistentvolumeclaim_status_phase{phase!="Bound"}[15m]) > 1
      annotations:
        severity: "HIGH"
        message: "{{$labels.namespace}}: {{$labels.persistentvolumeclaim}} is {{$labels.phase}}"
    - alert: node_filesystem_free_pv_percent
      expr: min by(device) (node_filesystem_free_bytes / node_filesystem_size_bytes{device!~"/dev.*pv.",device!~"/dev.*brick.*"} * 100) < 10
      annotations:
        severity: "HIGH"
        message: "{{$labels.instance}}: Available Space on {{$labels.mountpoint}} (current value is: {{$value}})%"

    - alert: node_swap_enabled
      expr: node_memory_SwapTotal != 0
      annotations:
        severity: "HIGH"
        message: "On {{$kubernetes_io_hostname}} swap enabled"

    - alert: node_filesystem_device_error
      expr: node_filesystem_device_error{fstype!="tmpfs",fstype!="fuse.glusterfs"} > 0
      annotations:
        severity: "HIGH"
        message: "{{$labels.instance}}: Errors on {{$labels.mountpoint}} detected"

    - alert: node_cpu_load5
      expr: max(node_load5)by (instance) > 8
      for: 30m
      annotations:
        severity: "HIGH"
        message: "{{$labels.instance}}: Load higher than (current value is: {{ $value }})"

    - alert: node_memory_free_percent
      expr: ((node_memory_MemTotal_bytes-node_memory_MemFree_bytes-node_memory_Cached_bytes)/node_memory_MemTotal_bytes)*100 > 90
      annotations:
        severity: "HIGH"
        message: "{{$labels.instance}}: Memory usage more than 85% (current value is: {{ $value }})%"

    - alert: node_procs_running
      expr: max(node_procs_running) by (instance) > 100
      annotations:
        severity: "HIGH"
        message: "{{$labels.instance}}: More than 100 active processes running (current value is: {{ $value }})"

    - alert: node_procs_blocked
      expr: rate(node_procs_blocked[2m]) > 10
      annotations:
        severity: "HIGH"
        message: "{{$labels.instance}}: {{ $value }} processes blocked waiting for I/O to complete"

  - name: custom_kubernetes_monitoring
    rules:
    - alert: docker_pool_usage
      expr: max(container_fs_usage_bytes{device="vg_docker-docker--pool",job="kubernetes-cadvisor",id="/"}) by (instance)/ max(container_fs_limit_bytes{device="vg_docker-docker--pool",id="/"}) by (instance)*100 > 90
      annotations:
        severity: "HIGH"
        message: "Docker pool usage on {{$labels.instance}}: {{ $value }}%"

    - alert: kube_docker_operations_errors
      expr: rate(kubelet_docker_operations_errors[5m]) > 3
      annotations:
        severity: "HIGH"
        message: "Docker operation error: {{$labels.operation_type}} on {{$labels.instance}} failed"

    - alert: kube_docker_operations_latency_seconds_container_stop_actions
      expr: kubelet_docker_operations_latency_microseconds{quantile="0.9", operation_type=~"stop_container"}/1000/1000 > 50
      for: 15m
      annotations:
        severity: "HIGH"
        message: "Docker container operation {{$labels.operation_type}} on {{$labels.instance}} took {{ $value }} Seconds"

    - alert: kube_docker_operations_latency_seconds_container_start_create_inspect_actions
      expr: kubelet_docker_operations_latency_microseconds{quantile="0.9", operation_type!~"stop_container" ,operation_type=~".*container" }/1000/1000 > 10
      for: 15m
      annotations:
        severity: "HIGH"
        message: "Docker container operation {{$labels.operation_type}} on {{$labels.instance}} took {{ $value }} Seconds"

    - alert: kube_docker_operations_latency_seconds_container_pull_image
      expr: kubelet_docker_operations_latency_microseconds{quantile="0.9", operation_type="pull_image"}/1000/1000 > 300
      for: 15m
      annotations:
        severity: "HIGH"
        message: "Docker container operation {{$labels.operation_type}} on {{$labels.instance}} took {{ $value }} Seconds"

    - alert: kube_docker_operations_latency_seconds
      expr: kubelet_docker_operations_latency_microseconds{operation_type!="pull_image",operation_type!~".*container",operation_type!~".*logs",quantile="0.9"} / 1000 / 1000 > 30
      for: 15m
      annotations:
        severity: "HIGH"
        message: "Docker container operation {{$labels.operation_type}} on {{$labels.instance}} took {{ $value }} Seconds"

    - alert: kube_node_status_DiskPressure
      expr: kube_node_status_condition{condition="DiskPressure",status="true"} == 1
      annotations:
        severity: "HIGH"
        message: "{{$labels.instance}}: Node has DiskPressure"

    - alert: kube_node_status_MemoryPressure
      expr: kube_node_status_condition{condition="MemoryPressure",status="true"} == 1
      annotations:
        severity: "HIGH"
        message: "{{$labels.instance}}: Node has MemoryPressure"

    - alert: kube_node_status_OutOfDisk
      expr: kube_node_status_condition{condition="OutOfDisk",status="true"} == 1
      annotations:
        severity: "HIGH"
        message: "{{$labels.instance}}: Node is OutOfDisk"

    - alert: kube_pod_container_status_waiting
      expr: rate(kube_pod_container_status_waiting[5m]) > 0
      for: 1d
      annotations:
        severity: "HIGH"
        message: "{{$labels.container}}: Container has waiting status longer than 5min"

    - alert: kubelet_pleg_relist_latency_seconds
      expr: kubelet_pleg_relist_latency_microseconds{quantile="0.9"}/1000000 > 5
      for: 15m
      annotations:
        severity: "HIGH"
        message: "{{$labels.instance}}: PLEG latency high. High PLEG latency is often related to disk I/O performance on the docker storage partition."

  - name: custom_etcd_monitoring
    rules:
    - alert: etcd_disk_wal_fsync_duration_seconds_sum
      expr: etcd_disk_wal_fsync_duration_seconds_sum > 100
      annotations:
        severity: "HIGH"
        message: "Etcd disk write-ahead-log latency more than 100 milliseconds"

    - alert: etcd_server_proposals_failed_total
      expr: rate(etcd_server_proposals_failed_total[5m]) > 0
      annotations:
        severity: "HIGH"
        message: "Etcd server proposal failed"

  - name: custom_router_monitoring
    rules:
    - alert: router_kube_pod_container_status_ready
      expr: kube_pod_container_status_ready{pod=~".*router.*",namespace="default"} != 1
      for: 15m
      annotations:
        severity: "HIGH"
        message: "{{$labels.pod}}: pod NotReady"

    - alert: router_reload_seconds
      expr: template_router_reload_seconds{quantile="0.9"} > 3
      annotations:
        severity: "HIGH"
        message: "Router reloading takes more than {{ $value }} seconds on {{$labels.instance}}"

    - alert: router_router_write_config_seconds
      expr: template_router_write_config_seconds{quantile="0.9"} > 3
      annotations:
        severity: "HIGH"
        message: "Router is writing to disk takes more than {{ $value }}  seconds on {{$labels.instance}}"

    - alert: router_http_request_duration_seconds
      expr: rate(http_request_duration_microseconds[10m])/1000000 > 0.5
      annotations:
        severity: "HIGH"
        message: "Router HTTP request latencies are higher than {{ $value }} seconds on {{$labels.instance}}"

    - alert: router_on_same_node
      expr: count(kube_pod_info{pod=~"router.*"})by (node) > 2
      annotations:
        severity: "HIGH"
        message: "{{$labels.pod}}:on same node"

  - name: custom_logging_monitoring
    rules:
    - alert: logging_es_container_count
      expr: count(kube_pod_container_status_ready{pod=~"logging-es-data-master.*",container="elasticsearch"}) < 3
      for: 5m
      annotations:
        severity: "HIGH"
        message: "Less than 3 logging-es pods running for 5min"

    - alert: logging_es_on_same_node
      expr: count(kube_pod_info{pod=~"logging-es.*"}) by (node)  > 1
      annotations:
        severity: "HIGH"
        message: "{{$labels.pod}}: running on same node"

    - alert: logging_es_cluster_is_timedout
      expr: es_cluster_is_timedout_bool != 0
      for: 5m
      annotations:
        severity: "HIGH"
        message: "{{$labels.instance}}: ElasticSearch cluster timeout"

    - alert: logging_es_cluster_status
      expr: es_cluster_status != 0
      for: 5m
      annotations:
        severity: "HIGH"
        message: "{{$labels.instance}}: ElasticSearch cluster status not ready"

    - alert: logging_es_fs_used
      expr: es_fs_total_free_bytes/es_fs_total_total_bytes*100 < 10
      for: 5m
      annotations:
        severity: "HIGH"
        message: "{{$labels.node}}: More than 90% space used"

    - alert: logging_es_mem_used
      expr: es_os_mem_free_bytes/es_os_mem_total_bytes*100 > 90
      for: 5m
      annotations:
        severity: "HIGH"
        message: "{{$labels.node}}: More than 90% memory used"

    - alert: logging_es_os_load_average
      expr: es_os_load_average > 5
      for: 1h
      annotations:
        severity: "HIGH"
        message: "{{$labels.node}}: High Load detected"

  - name: custom_registry_monitoring
    rules:
    - alert: registry_container_count
      expr: count(kube_pod_info{pod=~"docker-registry.*",pod!~".*-deploy"}) < 2
      for: 5m
      annotations:
        severity: "HIGH"
        message: "Less than 2 docker-registry running for 5min"

    - alert: registry_on_same_node
      expr: count by(node) (kube_pod_info{pod=~"docker-registry.*",pod!~".*-deploy"}) > 1
      annotations:
        severity: "HIGH"
        message: "{{$labels.pod_name}}: running on same node"
    
    ## TODO: Add blackbox exporter
    - alert: registry_http_auth
      expr: probe_http_status_code{instance="https://docker-registry.default.svc.cluster.local:5000/v2"} != 401
      for: 15m
      annotations:
        severity: "HIGH"
        message: "Docker-registry internal anonymous access denied"

  - name: custom_hawkular_monitoring
    rules:
    - alert: probe_http_status_metrics
      expr: probe_http_status_code{instance="https://metrics.ospa.pnet.ch/hawkular/metrics"} != 200
      for: 15m
      annotations:
        severity: "HIGH"
        message: "{{$labels.instance}}: unreachable "

  - name: custom_container_monitoring
    rules:
    - alert: container_cpu_usage_seconds_over_5min_default
      expr: rate(container_cpu_usage_seconds_total{namespace="default"}[5m]) > 0.5
      annotations:
        severity: "HIGH"
        message: "High CPU usage in {{$labels.namespace}} of {{$labels.pod_name}} {{ $value }}"

    - alert: container_cpu_usage_seconds_over_5min_openshift
      expr: rate(container_cpu_usage_seconds_total{namespace=~"openshift-.*"}[5m]) > 1
      annotations:
        severity: "HIGH"
        message: "High CPU usage in {{$labels.namespace}} of {{$labels.pod_name}} {{ $value }}"

    - alert: container_mem_usage_default_mb
      expr: container_memory_max_usage_bytes{namespace="default"}/1024/1024 > 2000
      annotations:
        severity: "HIGH"
        message: "Memory usage more than 100MB of {{$labels.pod_name}} in {{$labels.namespace}} project"

    - alert: container_mem_usage_openshift_infra_hawkular_cassandra_mb
      expr: container_memory_max_usage_bytes{namespace="openshift-infra",pod_name=~"hawkular-cassandra.*"}/1024/1024 > 1950
      annotations:
        severity: "HIGH"
        message: "Memory usage more than 1950MB of {{$labels.pod_name}} in {{$labels.namespace}} project"

    - alert: container_mem_usage_openshift_infra_hawkular_metrics_mb
      expr: container_memory_max_usage_bytes{namespace="openshift-infra",pod_name=~"hawkular-metrics.*"}/1024/1024 > 2400
      annotations:
        severity: "HIGH"
        message: "Memory usage more than 2400MB of {{$labels.pod_name}} in {{$labels.namespace}} project"

    - alert: pod_restart_count
      expr: rate(kube_pod_container_status_restarts_total[5m]) > 0
      for: 1h
      annotations:
        severity: "HIGH"
        message: "{{$labels.pod_name}}: Restarting"

  - name: custom_http_monitoring
    rules:
    - alert: http_probe_dns_lookup_time_seconds
      expr: probe_dns_lookup_time_seconds > 10
      annotations:
        severity: "HIGH"
        message: "{{$labels.instance}}: processing dns lookups takes {{ $value }} seconds"

    - alert: http_duration_seconds_processing
      expr: http_request_duration_microseconds/1000/1000 > 10
      for: 15m
      annotations:
        severity: "HIGH"
        message: "{{$labels.instance}}: http checks takes {{ $value }} seconds"

    - alert: http_probe_ssl_earliest_cert_expiry_days
      expr: (probe_ssl_earliest_cert_expiry - time())/60/60/24 < 20
      annotations:
        severity: "HIGH"
        message: "{{$labels.instance}} SSL cert expires in less than 90days"

    - alert: http_probe_http_status_code
      expr: probe_http_status_code == 0
      for: 15m
      annotations:
        severity: "HIGH"
        message: "{{$labels.instance}} not available"
